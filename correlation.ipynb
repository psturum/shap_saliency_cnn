{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPfz0T6hYiIv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "!pip install shap\n",
        "!pip install scikit-image\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "# Set a seed for reproducibility\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "6oU3-BkjfcYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reshape and normalize data\n",
        "x_train_mnist = x_train_mnist.reshape(60000, 28, 28, 1).astype(\"float32\") / 255\n",
        "x_test_mnist = x_test_mnist.reshape(10000, 28, 28, 1).astype(\"float32\") / 255"
      ],
      "metadata": {
        "id": "lsAP49eqfisJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Specify the file path\n",
        "file_path = '/content/drive/MyDrive/fmnist/training_history.pkl'\n",
        "\n",
        "# Load the pickled file\n",
        "with open(file_path, 'rb') as file:\n",
        "    training_history_mnist = pickle.load(file)\n",
        "\n",
        "# Specify the file path\n",
        "file_path = '/content/drive/MyDrive/fmnist_rand/training_history.pkl'\n",
        "\n",
        "# Load the pickled file\n",
        "with open(file_path, 'rb') as file:\n",
        "    training_history_mnist_rand = pickle.load(file)\n",
        "\n",
        "# Now, `training_history` contains the data stored in the pickled file"
      ],
      "metadata": {
        "id": "mEUTSZggXwhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming training_history is a dictionary\n",
        "loss_mnist = np.array(training_history_mnist[\"loss\"])\n",
        "val_loss_mnist = np.array(training_history_mnist[\"val_loss\"])\n",
        "\n",
        "# Assuming training_history is a dictionary\n",
        "loss_mnist_rand = np.array(training_history_mnist_rand[\"loss\"])\n",
        "val_loss_mnist_rand = np.array(training_history_mnist_rand[\"val_loss\"])\n",
        "\n",
        "# Plotting\n",
        "epochs = range(1, len(loss_mnist_rand) + 1)\n",
        "\n",
        "\n",
        "plt.plot(epochs, loss_mnist_rand, 'r-', label='fmnist_rand - train')\n",
        "plt.plot(epochs, val_loss_mnist_rand, 'b-', label='fmnist_rand - val')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lFawJ8X2Yg46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_shap_values = np.load(\"/content/drive/MyDrive/fmnist/shap_values_fmnist_single_tshirt.npz\")"
      ],
      "metadata": {
        "id": "MoCMgyhwY7Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_value_0 = loaded_shap_values['shap_value_0']\n",
        "shap_value_1 = loaded_shap_values['shap_value_1']\n",
        "shap_value_2 = loaded_shap_values['shap_value_2']\n",
        "shap_value_3 = loaded_shap_values['shap_value_3']\n",
        "shap_value_4 = loaded_shap_values['shap_value_4']\n",
        "shap_value_5 = loaded_shap_values['shap_value_5']\n",
        "shap_value_6 = loaded_shap_values['shap_value_6']\n",
        "shap_value_7 = loaded_shap_values['shap_value_7']\n",
        "shap_value_8 = loaded_shap_values['shap_value_8']\n",
        "shap_value_9 = loaded_shap_values['shap_value_9']"
      ],
      "metadata": {
        "id": "iKl62nTOZ2DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_value_0.shape"
      ],
      "metadata": {
        "id": "6G_nE6XoZ3sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming shap_value_0 is your 2D array\n",
        "# Create a custom colormap with blue for negative values and red for positive values\n",
        "cmap = plt.cm.RdBu_r\n",
        "\n",
        "# Plot the image with a custom colormap\n",
        "plt.imshow(shap_value_0, cmap=cmap, vmin=-np.max(np.abs(shap_value_0)), vmax=np.max(np.abs(shap_value_0)))\n",
        "\n",
        "# Set colorbar to show the correspondence between color and intensity\n",
        "plt.colorbar()\n",
        "\n",
        "# Hide axis labels\n",
        "plt.axis('off')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EuqIKTfIZ-5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the image\n",
        "plt.imshow(shap_value_3)  # Use 'gray' colormap for grayscale images\n",
        "plt.axis('off')  # Hide axis labels\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fsa8-xshagaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Specify the file path\n",
        "model_path_mnist = '/content/drive/MyDrive/mnist/your_model.keras'\n",
        "\n",
        "# Load the model\n",
        "loaded_model_mnist = load_model(model_path_mnist)\n",
        "\n",
        "# Specify the file path\n",
        "model_path_mnist_rand = '/content/drive/MyDrive/mnist_rand/your_model.keras'\n",
        "\n",
        "# Load the model\n",
        "loaded_model_mnist_rand = load_model(model_path_mnist_rand)\n",
        "\n",
        "\n",
        "# Now, `loaded_model` contains the Keras model loaded from the specified file\n"
      ],
      "metadata": {
        "id": "YEIPjVDne8K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_mnist.summary()"
      ],
      "metadata": {
        "id": "EPHB2Vo1fEn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save an example of each class from the test set\n",
        "x_test_dict = dict()\n",
        "for i, l in enumerate(y_test_mnist):\n",
        "  if len(x_test_dict)==10:\n",
        "    break\n",
        "  if l not in x_test_dict.keys():\n",
        "    x_test_dict[l] = x_test_mnist[i]\n",
        "\n",
        "# Convert to list preserving order of classes\n",
        "x_test_each_class = [x_test_dict[i] for i in sorted(x_test_dict)]\n",
        "\n",
        "# Convert to tensor\n",
        "x_test_each_class = np.asarray(x_test_each_class)\n",
        "\n",
        "# Print shape of tensor\n",
        "print(f\"x_test_each_class tensor has shape: {x_test_each_class.shape}\")"
      ],
      "metadata": {
        "id": "DX1Z9o9sftAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute predictions\n",
        "predictions = loaded_model_mnist.predict(x_test_each_class)\n",
        "\n",
        "# Apply argmax to get predicted class\n",
        "np.argmax(predictions, axis=1)"
      ],
      "metadata": {
        "id": "BlarvGisiXWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Specify the file path\n",
        "npz_path_mnist = '/content/drive/MyDrive/fmnist/shap_values_fmnist_single_tshirt.npz'\n",
        "\n",
        "# Load the NPZ file\n",
        "shap_values_mnist_2 = np.load(npz_path_mnist)\n",
        "\n",
        "\n",
        "# Specify the file path\n",
        "npz_path_mnist_rand = '/content/drive/MyDrive/fmnist_rand/shap_values_rand_fmnist_single_tshirt.npz'\n",
        "\n",
        "# Load the NPZ file\n",
        "shap_values_mnist_2_rand = np.load(npz_path_mnist_rand)\n",
        "\n",
        "# Now, `shap_values_mnist_2` contains the data loaded from the specified file\n"
      ],
      "metadata": {
        "id": "1vH8CxK8javC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select 5000 random samples from x_test\n",
        "background = x_test_mnist[np.random.choice(x_test_mnist.shape[0], 1500, replace=False)]"
      ],
      "metadata": {
        "id": "vpIFoUupUAD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DeepExplainer to explain predictions of the model\n",
        "e = shap.DeepExplainer(loaded_model_mnist, background)\n",
        "\n",
        "shap_values = e.shap_values(x_test_each_class[0].reshape(1, 28, 28, 1))\n",
        "shap.image_plot(shap_values, -x_test_each_class[0].reshape(1,28,28,1))"
      ],
      "metadata": {
        "id": "Q6NRAN78UIYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DeepExplainer to explain predictions of the model\n",
        "e_rand = shap.DeepExplainer(loaded_model_mnist_rand, background)\n",
        "\n",
        "shap_values_rand = e_rand.shap_values(x_test_each_class[0].reshape(1, 28, 28, 1))\n",
        "shap.image_plot(shap_values_rand, -x_test_each_class[0].reshape(1,28,28,1))"
      ],
      "metadata": {
        "id": "v2vqOOSCUWpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming shap_values_mnist_2 contains 10 (28, 28) arrays\n",
        "shap_values_list_mnist = [shap_values_mnist_2[f\"shap_value_{i}\"] for i in range(10)]\n",
        "shap_values_list_mnist_rand = [shap_values_mnist_2_rand[f\"shap_value_{i}\"] for i in range(10)]\n",
        "\n",
        "# Calculate the overall maximum and minimum values across all images for mnist\n",
        "max_intensity_mnist = np.max([np.max(np.abs(image)) for image in shap_values])\n",
        "min_intensity_mnist = -max_intensity_mnist\n",
        "\n",
        "# Calculate the overall maximum and minimum values across all images for mnist_rand\n",
        "max_intensity_mnist_rand = np.max([np.max(np.abs(image)) for image in shap_values_rand])\n",
        "min_intensity_mnist_rand = -max_intensity_mnist_rand\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
        "\n",
        "# Plot mnist\n",
        "for i, ax in enumerate(axes[0]):\n",
        "    ax.imshow(shap_values[i].reshape(28,28), cmap='bwr', vmin=min_intensity_mnist, vmax=max_intensity_mnist)\n",
        "    ax.axis('off')\n",
        "\n",
        "# Plot mnist_rand\n",
        "for i, ax in enumerate(axes[1]):\n",
        "    ax.imshow(shap_values_rand[i].reshape(28,28), cmap='bwr', vmin=min_intensity_mnist_rand, vmax=max_intensity_mnist_rand)\n",
        "    ax.axis('off')\n",
        "\n",
        "# Add gridlines for the entire subplot\n",
        "plt.grid(True, color='black', linewidth=0.5, linestyle='--', which='both', axis='both')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HaEkKt_3j1DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming shap_values_list_mnist is your list of (28, 28) saliency maps\n",
        "abs_sum_per_pixel_list = [np.sum(np.abs(saliency_map)) for saliency_map in shap_values_list_mnist]\n",
        "\n",
        "print(\"Absolute Sum of Each Pixel for Each Saliency Map:\")\n",
        "for i, abs_sum_per_pixel in enumerate(abs_sum_per_pixel_list):\n",
        "    print(f\"Element {i + 1}: {abs_sum_per_pixel}\")\n"
      ],
      "metadata": {
        "id": "tMeZCQFGj2G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.feature import hog\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def calculate_mean_correlation(shap_values_list_x, shap_values_list_y):\n",
        "    num_instances = len(shap_values_list_x)\n",
        "\n",
        "    # Normalize SHAP values before the loop\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    # Assuming shap_values_list_x and shap_values_list_y are your lists of (28, 28) saliency maps\n",
        "    normalized_shap_values_x = [scaler.fit_transform(shap_x.reshape(28,28)) for shap_x in shap_values_list_x]\n",
        "    normalized_shap_values_y = [scaler.transform(shap_y.reshape(28,28)) for shap_y in shap_values_list_y]\n",
        "\n",
        "    correlations_data = []\n",
        "\n",
        "    for i in range(num_instances):\n",
        "        shap_x = normalized_shap_values_x[i].reshape(28,28)\n",
        "        shap_y = normalized_shap_values_y[i].reshape(28,28)\n",
        "\n",
        "        # Calculate Pearson correlation\n",
        "        correlation_coefficient_pearson, p1 = pearsonr(shap_x.flatten(), shap_y.flatten())\n",
        "        # Calculate Spearman rank correlation\n",
        "        correlation_coefficient_spearman, p2 = spearmanr(shap_x.flatten(), shap_y.flatten())\n",
        "        # Calculate SSIM\n",
        "        ssim_index, _ = ssim(shap_x, shap_y, full=True)\n",
        "\n",
        "        # Calculate HOG\n",
        "        hog_feature_x, _ = hog(shap_x, visualize=True)\n",
        "        hog_feature_y, _ = hog(shap_y, visualize=True)\n",
        "        hog_coef, _ = pearsonr(hog_feature_x.flatten(), hog_feature_y.flatten())\n",
        "\n",
        "        # Append correlations for each element\n",
        "        correlations_data.append({\n",
        "            'Pearson': correlation_coefficient_pearson,\n",
        "            'Spearman': correlation_coefficient_spearman,\n",
        "            'SSIM': ssim_index,\n",
        "            'HOG': hog_coef\n",
        "        })\n",
        "\n",
        "    # Calculate mean coefficients\n",
        "    mean_pearson = np.mean([entry['Pearson'] for entry in correlations_data])\n",
        "    mean_spearman = np.mean([entry['Spearman'] for entry in correlations_data])\n",
        "    mean_ssim = np.mean([entry['SSIM'] for entry in correlations_data])\n",
        "    mean_hog = np.mean([entry['HOG'] for entry in correlations_data])\n",
        "\n",
        "    # Add a row for mean correlations in the DataFrame\n",
        "    correlations_data.append({\n",
        "        'Pearson': mean_pearson,\n",
        "        'Spearman': mean_spearman,\n",
        "        'SSIM': mean_ssim,\n",
        "        'HOG': mean_hog\n",
        "    })\n",
        "\n",
        "    # Create a DataFrame\n",
        "    correlations_df = pd.DataFrame(correlations_data)\n",
        "    return correlations_df\n"
      ],
      "metadata": {
        "id": "z5yAnPB0pX1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "element_and_mean_correlations = calculate_mean_correlation(shap_values, shap_values_rand)"
      ],
      "metadata": {
        "id": "unIR8TueHAYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "element_and_mean_correlations"
      ],
      "metadata": {
        "id": "fHN6LPY3rN9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming correlations_df is your DataFrame with mean coefficients\n",
        "mean_coefficients = element_and_mean_correlations.iloc[2, :4]  # Assuming your DataFrame has the metrics in columns 1 and onwards\n",
        "\n",
        "# Plotting with a narrower figure\n",
        "fig, ax = plt.subplots(figsize=(4, 2))\n",
        "\n",
        "# Define metrics, colors, and labels\n",
        "metrics = ['Pearson', 'Spearman', 'SSIM', 'HOG']\n",
        "colors = ['blue', 'green', 'orange', 'red']\n",
        "labels = ['Pearson', 'Spearman', 'SSIM', 'HOG']\n",
        "\n",
        "# Create bars with labels and colors\n",
        "bars = ax.bar(metrics, mean_coefficients, color=colors, label=labels, width=0.2)  # Adjusted width to 0.2\n",
        "\n",
        "ax.set_title('Correlation(C:2) fmnist - fmnist_rand')\n",
        "ax.set_xlabel('Metrics')\n",
        "ax.set_ylabel('Coefficient Value')\n",
        "\n",
        "\n",
        "# Display the bar plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BLXiXB1lv9e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(\n",
        "    42\n",
        ")\n",
        "\n",
        "# Function to reset weights for a specific layer\n",
        "def reset_layer_weights(layer):\n",
        "    layer.build(layer.input_shape)\n",
        "    return layer\n",
        "\n",
        "# Reset weights for specific layers and create new models\n",
        "layers_to_reset = ['dense_3', 'dense_2', 'conv2d_3', 'conv2d_2']\n",
        "reset_models = []\n",
        "\n",
        "for layer_name in layers_to_reset:\n",
        "    print(layer_name)\n",
        "    # Create a copy of the original model\n",
        "    reset_model = keras.models.clone_model(loaded_model_mnist)\n",
        "\n",
        "    # Find the index of the layer to reset\n",
        "    layer_index = [i for i, layer in enumerate(reset_model.layers) if layer.name == layer_name][0]\n",
        "\n",
        "    # Reset weights for the specific layer\n",
        "    reset_model.layers[layer_index] = reset_layer_weights(reset_model.layers[layer_index])\n",
        "    print(reset_model.layers[layer_index])\n",
        "    # Compile the reset model\n",
        "    reset_model.compile(\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "    # Append the reset model to the list\n",
        "    reset_models.append(reset_model)\n"
      ],
      "metadata": {
        "id": "F9V0jBaD6Cot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute predictions\n",
        "predictions = reset_models[0].predict(x_test_each_class)\n",
        "\n",
        "# Apply argmax to get predicted class\n",
        "np.argmax(predictions, axis=1)"
      ],
      "metadata": {
        "id": "XxI02u7b6XpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select 5000 random samples from x_test\n",
        "background = x_test_mnist[np.random.choice(x_test_mnist.shape[0], 1500, replace=False)]\n",
        "\n",
        "# Use DeepExplainer to explain predictions of the model\n",
        "e = shap.DeepExplainer(loaded_model_mnist, background)\n"
      ],
      "metadata": {
        "id": "s1JL0XaQCSuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_values = e.shap_values(x_test_each_class[0].reshape(1, 28, 28, 1))\n",
        "shap.image_plot(shap_values, -x_test_each_class[0].reshape(1,28,28,1))"
      ],
      "metadata": {
        "id": "e0w1IcYVEuYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming shap_values_list_mnist is your list of (28, 28) saliency maps\n",
        "abs_sum_per_pixel_list = [np.sum(np.abs(saliency_map)) for saliency_map in shap_values]\n",
        "\n",
        "print(\"Absolute Sum of Each Pixel for Each Saliency Map:\")\n",
        "for i, abs_sum_per_pixel in enumerate(abs_sum_per_pixel_list):\n",
        "    print(f\"Element {i + 1}: {abs_sum_per_pixel}\")\n"
      ],
      "metadata": {
        "id": "rDKF6PklczJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DeepExplainer to explain predictions of the model\n",
        "e_2 = shap.DeepExplainer(reset_models[0], background)\n",
        "shap_values_2 = e_2.shap_values(x_test_each_class[0].reshape(1, 28, 28, 1))\n",
        "shap.image_plot(shap_values_2, -x_test_each_class[0].reshape(1,28,28,1))"
      ],
      "metadata": {
        "id": "_qDMN-OKFSE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DeepExplainer to explain predictions of the model\n",
        "e_3 = shap.DeepExplainer(reset_models[1], background)\n",
        "shap_values_3 = e_3.shap_values(x_test_each_class[0].reshape(1, 28, 28, 1))\n",
        "shap.image_plot(shap_values_3, -x_test_each_class[0].reshape(1,28,28,1))"
      ],
      "metadata": {
        "id": "Rwe6obCfIlWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DeepExplainer to explain predictions of the model\n",
        "e_4 = shap.DeepExplainer(reset_models[2], background)\n",
        "shap_values_4 = e_4.shap_values(x_test_each_class[0].reshape(1, 28, 28, 1))\n",
        "shap.image_plot(shap_values_4, -x_test_each_class[0].reshape(1,28,28,1))"
      ],
      "metadata": {
        "id": "frcDpyjNIp9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DeepExplainer to explain predictions of the model\n",
        "e_5 = shap.DeepExplainer(reset_models[3], background)\n",
        "shap_values_5 = e_5.shap_values(x_test_each_class[0].reshape(1, 28, 28, 1))\n",
        "shap.image_plot(shap_values_5, -x_test_each_class[0].reshape(1,28,28,1))"
      ],
      "metadata": {
        "id": "_yOlhrrrIvf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "element_and_mean_correlations_1 = calculate_mean_correlation(shap_values, shap_values_2)\n",
        "element_and_mean_correlations_2 = calculate_mean_correlation(shap_values, shap_values_3)\n",
        "element_and_mean_correlations_3 = calculate_mean_correlation(shap_values, shap_values_4)\n",
        "element_and_mean_correlations_4 = calculate_mean_correlation(shap_values, shap_values_5)"
      ],
      "metadata": {
        "id": "kzWSkVtXJPYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "element_and_mean_correlations_1"
      ],
      "metadata": {
        "id": "kTAbDI12nAzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "element_and_mean_correlations_2"
      ],
      "metadata": {
        "id": "7KS21Jb0nCa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "element_and_mean_correlations_3"
      ],
      "metadata": {
        "id": "MVOmHt64U37K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "element_and_mean_correlations_4"
      ],
      "metadata": {
        "id": "OKpFENyRU4eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hog_coefficients_list = [1.0]\n",
        "\n",
        "# Assuming element_and_mean_correlations_i represents the dataframes for i = 1 to 4\n",
        "for i in range(1, 5):\n",
        "    print(f\"element_and_mean_correlations_{i}\")\n",
        "    df_name = f\"element_and_mean_correlations_{i}\"\n",
        "    hog_coefficient = globals()[df_name].loc[0, 'Spearman']\n",
        "    hog_coefficients_list.append(hog_coefficient)\n",
        "\n",
        "print(\"HOG Coefficients List:\", hog_coefficients_list)\n",
        "\n",
        "hog_coefficients_list_second = [1.0]\n",
        "\n",
        "# Assuming element_and_mean_correlations_i represents the dataframes for i = 1 to 4\n",
        "for i in range(1, 5):\n",
        "    df_name = f\"element_and_mean_correlations_{i}\"\n",
        "    hog_coefficient_second = globals()[df_name].loc[2, 'Spearman']\n",
        "    hog_coefficients_list_second.append(hog_coefficient_second)\n",
        "\n",
        "print(\"HOG Coefficients List:\", hog_coefficients_list_second)"
      ],
      "metadata": {
        "id": "ORXgmAs0lEgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming hog_coefficients_list contains values for HOG coefficients\n",
        "\n",
        "# Labels for ticks\n",
        "tick_labels = ['dense2', 'dense1', 'hidden2', 'hidden1']\n",
        "\n",
        "# Plotting with striped black line and black dots\n",
        "plt.plot(hog_coefficients_list, marker='o', linestyle='--', color='black')\n",
        "plt.plot(hog_coefficients_list_second, marker='o', linestyle='--', color='blue')\n",
        "plt.xticks(range(len(tick_labels)), tick_labels)\n",
        "plt.title('HOG fmnist independent')\n",
        "plt.xlabel('Layers')\n",
        "plt.ylabel('HOG Coefficient')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM_aTDz4O8ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Function to reset weights for a specific layer\n",
        "def reset_layer_weights(layer):\n",
        "    layer.build(layer.input_shape)\n",
        "    return layer\n",
        "\n",
        "# Reset weights for specific layers and create new models\n",
        "layers_to_reset = ['dense_3', 'dense_2', 'conv2d_3', 'conv2d_2']\n",
        "reset_models_cascading = []\n",
        "\n",
        "for i in range(len(layers_to_reset) + 1):\n",
        "    # Create a copy of the original model\n",
        "    reset_model = keras.models.clone_model(loaded_model_mnist)\n",
        "\n",
        "    # Reset weights for the specific layers up to the current index\n",
        "    for layer_name in layers_to_reset[:i]:\n",
        "        layer_index = [j for j, layer in enumerate(reset_model.layers) if layer.name == layer_name][0]\n",
        "        reset_model.layers[layer_index] = reset_layer_weights(reset_model.layers[layer_index])\n",
        "\n",
        "    # Compile the reset model\n",
        "    reset_model.compile(\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "    # Append the reset model to the list\n",
        "    reset_models_cascading.append(reset_model)\n",
        "\n",
        "# Now reset_models_cascading contains four models with cascading layer resets\n"
      ],
      "metadata": {
        "id": "g8kiR41cCRNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DeepExplainer to explain predictions of the model\n",
        "e_2_casc = shap.DeepExplainer(reset_models_cascading[0], background)\n",
        "shap_values_2_casc = e_2_casc.shap_values(x_test_each_class[0].reshape(1, 28, 28, 1))\n",
        "shap.image_plot(shap_values_2_casc, -x_test_each_class[0].reshape(1,28,28,1))"
      ],
      "metadata": {
        "id": "YKKkArka88Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DeepExplainer to explain predictions of the model\n",
        "e_3_casc = shap.DeepExplainer(reset_models_cascading[1], background)\n",
        "shap_values_3_casc = e_3_casc.shap_values(x_test_each_class[0].reshape(1, 28, 28, 1))\n",
        "shap.image_plot(shap_values_3_casc, -x_test_each_class[0].reshape(1,28,28,1))"
      ],
      "metadata": {
        "id": "uAMHJ1xP8-x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DeepExplainer to explain predictions of the model\n",
        "e_4_casc = shap.DeepExplainer(reset_models_cascading[2], background)\n",
        "shap_values_4_casc = e_4_casc.shap_values(x_test_each_class[0].reshape(1, 28, 28, 1))\n",
        "shap.image_plot(shap_values_4_casc, -x_test_each_class[0].reshape(1,28,28,1))"
      ],
      "metadata": {
        "id": "N0bPW1nB9B8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DeepExplainer to explain predictions of the model\n",
        "e_5_casc = shap.DeepExplainer(reset_models_cascading[3], background)\n",
        "shap_values_5_casc = e_5_casc.shap_values(x_test_each_class[0].reshape(1, 28, 28, 1))\n",
        "shap.image_plot(shap_values_5_casc, -x_test_each_class[0].reshape(1,28,28,1))"
      ],
      "metadata": {
        "id": "6WxNznGA9Eid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "element_and_mean_correlations_1_casc = calculate_mean_correlation(shap_values, shap_values_2_casc)\n",
        "element_and_mean_correlations_2_casc = calculate_mean_correlation(shap_values, shap_values_3_casc)\n",
        "element_and_mean_correlations_3_casc = calculate_mean_correlation(shap_values, shap_values_4_casc)\n",
        "element_and_mean_correlations_4_casc = calculate_mean_correlation(shap_values, shap_values_5_casc)"
      ],
      "metadata": {
        "id": "zLNEd7uk-FOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "element_and_mean_correlations_1_casc"
      ],
      "metadata": {
        "id": "DdFFT6_qmkiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "element_and_mean_correlations_2_casc"
      ],
      "metadata": {
        "id": "u6Aan8ALml_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "element_and_mean_correlations_3_casc"
      ],
      "metadata": {
        "id": "KAxxc_QWms3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "element_and_mean_correlations_4_casc"
      ],
      "metadata": {
        "id": "z9SLxSQ4muyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hog_coefficients_list_casc = [1.0]\n",
        "\n",
        "# Assuming element_and_mean_correlations_i represents the dataframes for i = 1 to 4\n",
        "for i in range(1, 5):\n",
        "    df_name_casc = f\"element_and_mean_correlations_{i}_casc\"\n",
        "    hog_coefficient_casc = globals()[df_name_casc].loc[0, 'Spearman']\n",
        "    hog_coefficients_list_casc.append(hog_coefficient_casc)\n",
        "\n",
        "print(\"HOG Coefficients List:\", hog_coefficients_list_casc)\n",
        "\n",
        "hog_coefficients_list_casc_second = [1.0]\n",
        "\n",
        "# Assuming element_and_mean_correlations_i represents the dataframes for i = 1 to 4\n",
        "for i in range(1, 5):\n",
        "    df_name_casc_second = f\"element_and_mean_correlations_{i}_casc\"\n",
        "    hog_coefficient_casc_second = globals()[df_name_casc_second].loc[2, 'Spearman']\n",
        "    hog_coefficients_list_casc_second.append(hog_coefficient_casc_second)\n",
        "\n",
        "print(\"HOG Coefficients List:\", hog_coefficients_list_casc_second)"
      ],
      "metadata": {
        "id": "C7goy7DqliFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming hog_coefficients_list contains values for HOG coefficients\n",
        "\n",
        "# Labels for ticks\n",
        "tick_labels = ['original', 'dense2', 'dense1', 'hidden2', 'hidden1']\n",
        "\n",
        "# Plotting with striped black line and black dots\n",
        "plt.plot(hog_coefficients_list_casc, label = \"Cascade\", marker='o', linestyle='--', color='black')\n",
        "plt.plot(hog_coefficients_list, marker='o', label = \"Independent\", linestyle='--', color='green')\n",
        "plt.xticks(range(len(tick_labels)), tick_labels)\n",
        "plt.title('Spearman mnist')\n",
        "plt.xlabel('Layers')\n",
        "plt.ylim(0.0)\n",
        "plt.ylabel('Spearman Coefficient')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i832L6oHmHRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr, spearmanr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.feature import hog\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_mean_correlation_1(shap_values_list_x, shap_values_list_y, element_index):\n",
        "    num_instances = len(shap_values_list_x)\n",
        "    correlations_data = []\n",
        "\n",
        "    for i in range(num_instances):\n",
        "\n",
        "        shap_x = shap_values_list_x[i].reshape((28, 28))\n",
        "        shap_y = shap_values_list_y[i].reshape((28, 28))\n",
        "        # Calculate SSIM\n",
        "        ssim_index, ssim_image = ssim(shap_x, shap_y, full=True)\n",
        "\n",
        "        # Only visualize SSIM image for the specified element (i=4)\n",
        "        if i == element_index:\n",
        "            plt.figure(figsize=(12, 4))\n",
        "\n",
        "            # Plot SHAP values\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.imshow(shap_x, cmap='bwr', vmin=-np.max(np.abs(shap_x)), vmax=np.max(np.abs(shap_x)))\n",
        "            plt.title(f'SHAP Value true - {i}')\n",
        "            plt.colorbar()\n",
        "\n",
        "            # Plot SHAP values\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.imshow(shap_y, cmap='bwr', vmin=-np.max(np.abs(shap_y)), vmax=np.max(np.abs(shap_y)))\n",
        "            plt.title(f'SHAP Value random - {i}')\n",
        "            plt.colorbar()\n",
        "\n",
        "            # Plot SSIM image\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.imshow(ssim_image, cmap='viridis', vmin=0, vmax=1)\n",
        "            plt.title(f'SSIM Image - {i}')\n",
        "            plt.colorbar()\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "        # Append correlations for each element with a name\n",
        "        correlations_data.append({\n",
        "            'Name': f'{i}',\n",
        "            'SSIM': ssim_index\n",
        "        })\n",
        "\n",
        "    # Calculate mean coefficients\n",
        "    mean_ssim = np.mean([entry['SSIM'] for entry in correlations_data])\n",
        "\n",
        "    # Add a row for mean correlations with a name\n",
        "    correlations_data.append({\n",
        "        'Name': 'Mean',\n",
        "        'SSIM': mean_ssim\n",
        "    })\n",
        "\n",
        "    # Create a DataFrame with names\n",
        "    correlations_df = pd.DataFrame(correlations_data).set_index('Name')\n",
        "\n",
        "    print(\"Correlations for Each Element and Mean:\")\n",
        "    print(correlations_df)\n",
        "\n",
        "    return correlations_df\n",
        "\n",
        "# Usage\n",
        "element_and_mean_correlations = calculate_mean_correlation(shap_values_list_mnist, shap_values_list_mnist_rand, element_index=0)\n"
      ],
      "metadata": {
        "id": "LKUUr4DDwWQ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}